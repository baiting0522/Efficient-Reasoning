# -*- coding: utf-8 -*-
"""Toy example

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uWhKEuqSlPknzsVzuD8M0u557iQAmZEa

# import and pre-process the dataset
"""

!pip install jsonlines torch

import jsonlines
import re
import torch

# Define function to extract question and final answer as a dictionary
def extract_problem_and_answer(entry):
    """Ensure function returns a dictionary with 'question' and 'final_answer'."""
    question = entry["question"]

    # Extract final answer after '####'
    final_answer_match = re.search(r'####\s*(\S+)', entry["answer"])
    final_answer = final_answer_match.group(1) if final_answer_match else None

    return {  # Return dictionary
        "question": question,
        "final_answer": final_answer
    }

# Open and read JSONL file into a list
with jsonlines.open('/test.jsonl') as reader:
    data_list = list(reader)  # Read all data at once for batch processing

# Convert list into structured dictionary format
extracted_data = [extract_problem_and_answer(entry) for entry in data_list]

# Extract questions and answers separately for GPU processing
questions = [entry["question"] for entry in extracted_data]
answers = [entry["final_answer"] for entry in extracted_data]

# Print extracted data
#for item in extracted_data:
#    print(f"Question: {item['question']}")
#    print(f"Final Answer: {item['final_answer']}")
#    print("-" * 40)

!pip install huggingface_hub

import json
import torch
import torch.nn.functional as F
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import random
from transformers import AutoTokenizer, AutoModelForCausalLM

#config_data = json.load(open("/config.json"))
#HF_TOKEN = config_data["HF_TOKEN"]

from huggingface_hub import login

login(token="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")

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)  # Move model to GPU

# Split the dataset into calibration and test sets (80% calibration, 20% test)
random.shuffle(extracted_data)  # Shuffle the data for randomness
split_index = int(0.8 * len(extracted_data))
calibration_data = extracted_data[:2]
test_data = extracted_data[split_index:split_index+10]

import numpy as np

"""### Calculate confidence measure for calibration set
##### confidence: If I stop reasoning at step t, how likely is the answer the same as the final answer if I complete all reasoning steps?
"""

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

def compute_sentence_level_confidence(question, model, tokenizer, device, sim_model=None, stop_threshold=0.9):
    """Generate reasoning steps and compute confidence at the end of each sentence."""
    inputs = tokenizer(question, return_tensors="pt").to(device)

    # Generate full reasoning chain
    with torch.no_grad():
        outputs = model.generate(
            **inputs, max_new_tokens=1000, return_dict_in_generate=True,
            output_scores=True, pad_token_id=tokenizer.eos_token_id
        )

    full_answer = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)

    # Sentence segmentation
    def split_into_sentences(text):
        return re.split(r'(?<=[.!?])\s+', text.strip())  # Split by punctuation

    full_sentences = split_into_sentences(full_answer)
    accumulated_sentences = []
    confidence_scores = []

    for i in range(len(full_sentences)):
        accumulated_sentences.append(full_sentences[i])
        intermediate_answer = " ".join(accumulated_sentences)

        # Compute similarity between truncated answer and full answer
        if sim_model:
            embedding_inter = sim_model.encode([intermediate_answer])
            embedding_full = sim_model.encode([full_answer])
            similarity = cosine_similarity(embedding_inter, embedding_full)[0, 0]
        else:
            similarity = 1.0 if intermediate_answer.strip() == full_answer.strip() else 0.0

        # Confidence = similarity score (since we measure stopping likelihood)
        confidence_scores.append(similarity)

    return confidence_scores, intermediate_answer, full_answer

def compute_calibration_confidences(calibration_data, model, tokenizer, device):
    """Compute confidence scores for the calibration set."""

    all_confidences = []
    max_steps = 0

    # Compute confidence scores for each question
    for entry in calibration_data:
        question = entry["question"]
        confidence_scores, _, _ = compute_sentence_level_confidence(question, model, tokenizer, device)
        all_confidences.append(confidence_scores)
        max_steps = max(max_steps, len(confidence_scores))

    # Create a NumPy array with NaN padding
    confidence_matrix = np.full((len(all_confidences), max_steps), np.nan)

    # Fill the matrix with confidence scores
    for i, scores in enumerate(all_confidences):
        confidence_matrix[i, :len(scores)] = scores

    return confidence_matrix, max_steps

# Compute calibration confidence scores
calibration_confidences, max_steps = compute_calibration_confidences(calibration_data, model, tokenizer, device, return_legacy_cache=True)

import numpy as np

def compute_conformal_thresholds(confidences, alpha=0.1):
    thresholds = []

    for step in range(confidences.shape[1]):  # Iterate over each sentence-level step
        step_confidences = confidences[:, step]
        step_confidences = step_confidences[~np.isnan(step_confidences)]  # Remove NaNs

        if len(step_confidences) > 0:
            threshold = np.quantile(step_confidences, 1 - alpha)  # Compute (1-alpha) quantile
        else:
            threshold = 0  # Default threshold if no data

        thresholds.append(threshold)

    return thresholds

# Compute conformal thresholds for each step
alpha = 0.1  # Coverage level
conformal_thresholds = compute_conformal_thresholds(calibration_confidences, alpha)

# Apply adaptive stopping on test data
def adaptive_reasoning(test_data, conformal_thresholds):
    results = []

    for entry in test_data:
        question = entry["question"]
        confidence_scores, full_answer = compute_final_answer_confidence(question)

        stop_step = len(confidence_scores) - 1  # Default to last step
        final_output = full_answer

        for step, confidence in enumerate(confidence_scores):  # Only one value per step
            if step < len(conformal_thresholds) and confidence <= conformal_thresholds[step]:
                stop_step = step
                final_output = tokenizer.decode(
                model.generate(
                    tokenizer(question, return_tensors="pt").to(device).input_ids,
                    max_new_tokens=step + 1, pad_token_id=tokenizer.eos_token_id
                    )[0],
                    skip_special_tokens=True)
        break

        # Extract only the numerical part of the final answer
        final_output_number = "".join(filter(str.isdigit, final_output)) or final_output.strip()
        results.append({
            "question": question,
            "stopped_at_step": stop_step,
            "final_answer": final_output_number
        })

    return results

# Run adaptive reasoning on test data
test_results = adaptive_reasoning(test_data, conformal_thresholds)

# Display first few results
for res in test_results[:5]:
    print(res)



"""### train a classifier to predict the probability that an early-stopped answer will match the final full answer"""

from sklearn.model_selection import train_test_split

def split_calibration_data(calibration_data):
    """
    Split the calibration data into training and calibration sets (50%-50%).

    """
    train_data, cal_data = train_test_split(calibration_data, test_size=0.5, random_state=42)
    return train_data, cal_data

# Split into train and calibration sets
train_data, cal_data = split_calibration_data(calibration_data)

# Load a sentence embedding model for feature extraction
from sentence_transformers import SentenceTransformer
#sim_model = SentenceTransformer("all-MiniLM-L6-v2").to(device)
extract_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B").to(device)
extract_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

def extract_answers_and_labels(data, model, tokenizer, device):
    answers = []
    labels = []

    for entry in data:
        question = entry["question"]

        # Generate full reasoning-based answer
        inputs = tokenizer(question, return_tensors="pt", truncation=True, max_length=8192).to(device)
        with torch.no_grad():
            output = model.generate(**inputs, max_new_tokens=8192, pad_token_id=tokenizer.eos_token_id)

        full_answer = tokenizer.decode(output[0], skip_special_tokens=True)

        # Extract final answer using a prompt
        prompt_final_answer = f"""
                    Given the following reasoning-based solution:
                    "{full_answer}"

                    Extract only the final numerical or logical answer.
                    Provide only the answer, without explanation.
                    """
        inputs = extract_tokenizer(prompt_final_answer, return_tensors="pt", truncation=True, max_length=8192).to(device)

        with torch.no_grad():
            final_output = extract_model.generate(**inputs, max_new_tokens=8192, pad_token_id=tokenizer.eos_token_id)

        final_answer = tokenizer.decode(final_output[0], skip_special_tokens=True).strip()

        # Store the full reasoning answer and final answer as labels
        answers.append(full_answer)
        labels.append(final_answer)

    return answers, labels

full_reasoning, final_answer = extract_answers_and_labels(train_data, model, tokenizer, device)

def extract_features_and_labels(data, model, tokenizer, device):
    """
    Extracts reasoning text at each step, intermediate answers, and labels (1 if consistent with final answer, 0 otherwise).

    Args:
        data (list of dict): Dataset where each entry contains 'reasoning' and 'final_answer'.
        extract_model: The model used to extract structured reasoning steps.
        extract_tokenizer: Tokenizer for the extraction model.
        device: The computing device (CPU/GPU).

    Returns:
        features (list of lists): Accumulated reasoning solutions at each step.
        intermediate_answers (list of lists): Intermediate answers extracted at each step.
        labels (list of lists): Binary labels (1 = consistent with final answer, 0 = inconsistent).
    """
    features = []
    intermediate_answers = []
    labels = []
    step_indices = []
    full_reasoning, final_answer = extract_answers_and_labels(data, model, extract_tokenizer, device)

    for full_reasoning, final_answer in zip(full_reasoning, final_answer):

        # Construct the structured prompt to extract reasoning steps and intermediate answers
        prompt = f"""
        You are given a reasoning-based mathematical solution.

        Solution: "{full_reasoning}"

        ### Task:
        Your task is to extract and structure the reasoning step-by-step.

        For each step:
        1. **Extract the accumulated reasoning text up to this step.** This includes all logical steps leading to the current stage of the solution.
        2. **Extract the intermediate answer inferred at this step.** This is the numerical or logical result derived from the reasoning at this point.

        ### How to Identify Distinct Mathematical Solutions:
        A solution is considered distinct if:
        1. **Different Methods:** The approach differs fundamentally (e.g., algebraic vs. geometric reasoning).
        2. **Different Intermediate Steps:** Even if the final result is the same, varying step sequences make the solutions different.
        3. **Different Assumptions:** Solutions based on different premises or conditions should be considered distinct.
        4. **Generalization vs. Specificity:** A more generalizable approach differs from a solution that applies only in specific cases.
        5. **Complexity Difference:** A significantly simpler or more complex approach is distinct, even if it leads to the same result.

        ### **Strict Output Format:**
        Ensure your response strictly follows this structured format:

        Format your output strictly as follows:
        Step 1:
        Reasoning: [Full reasoning up to step 1]
        Intermediate Answer: [Answer inferred at this step]

        Step 2:
        Reasoning: [Full reasoning up to step 2]
        Intermediate Answer: [Answer inferred at this step]

        ...

        Step N:
        Reasoning: [Full reasoning up to step N]
        Intermediate Answer: [Answer inferred at this step]

        ### **Important Rules:**
        - **DO NOT** include explanations, justifications, or extra text outside the required format.
        - **DO NOT** summarize; extract the exact reasoning steps as they evolve.
        - **ENSURE** that the **Intermediate Answer** strictly reflects what is derived from the reasoning at that step.
        - **DO NOT SKIP steps**; maintain logical flow from the first step to the final conclusion.

        **Follow these instructions precisely to ensure accurate extraction.**

        """

        # Generate response using extract_model
        inputs = extract_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=8192).to(device)
        with torch.no_grad():
            output = extract_model.generate(**inputs, max_new_tokens=8192, pad_token_id=extract_tokenizer.eos_token_id)

        extracted_text = extract_tokenizer.decode(output[0], skip_special_tokens=True).strip()

        # Parse extracted text into structured reasoning steps and answers
        reasoning_steps = []
        answers = []
        step_labels = []
        indices = []

        current_step = 0
        lines = extracted_text.split("\n")

        for line in lines:
            line = line.strip()
            if line.startswith("Step"):
                current_step += 1  # increment step counter
            elif line.startswith("Reasoning:"):
                reasoning = line[len("Reasoning:"):].strip()
                reasoning_steps.append(reasoning)
                indices.append(current_step)  # record step index
            elif line.startswith("Intermediate Answer:"):
                intermediate_answer = line[len("Intermediate Answer:"):].strip()
                answers.append(intermediate_answer)

                # Label consistency with final answer
                label = 1 if intermediate_answer == final_answer else 0
                step_labels.append(label)

        features.append(reasoning_steps)
        intermediate_answers.append(answers)
        labels.append(step_labels)
        step_indices.append(indices)

    return features, labels, step_indices

# Generate training features and labels
X_train, y_train = extract_features_and_labels(train_data, model, tokenizer, device)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

def train_early_stop_classifier(X_train, y_train):
    # Convert labels to NumPy array
    y_train = np.array(y_train)

    # Create a TF-IDF + Classifier pipeline
    model = Pipeline([
        ("tfidf", TfidfVectorizer(ngram_range=(1, 2))),  # Use unigram + bigram features
        ("classifier", LogisticRegression(random_state=42, max_iter=1000))
    ])

    # Train the classifier
    model.fit(X_train, y_train)

    return model

classifier = train_early_stop_classifier(X_train, y_train)

import numpy as np

def compute_conformal_threshold(calibration_data, classifier, model, tokenizer, device, alpha=0.1):
    worst_case_scores = []

    for entry in calibration_data:
        question = entry["question"]

        # Generate intermediate answers from the model
        inputs = tokenizer(question, return_tensors="pt").to(device)
        output = model.generate(**inputs, max_length=512, pad_token_id=tokenizer.eos_token_id)
        full_generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)

        # Split full-generated answer into sentences
        def split_sentences(text):
            return re.split(r'(?<=[.!?])\s+', text.strip())

        full_sentences = split_sentences(full_generated_answer)
        intermediate_answers = []
        accumulated_sentences = []

        for i in range(len(full_sentences)):
            accumulated_sentences.append(full_sentences[i])
            intermediate_answers.append(" ".join(accumulated_sentences))

        # Predict stopping probabilities using the trained classifier
        stopping_probs = classifier.predict_proba(intermediate_answers)[:, 1]  # Probability of stopping (class=1)

        # Identify inconsistent steps where stopping probability is low (model is uncertain)
        threshold = 1.0  # Define a threshold for inconsistency (can be tuned)
        inconsistent_steps = [i for i, prob in enumerate(stopping_probs) if prob < threshold]

        if inconsistent_steps:
            # Compute worst-case stopping probability among inconsistent steps
            worst_case_confidence = max(stopping_probs[i] for i in inconsistent_steps)
            worst_case_scores.append(worst_case_confidence)

    if not worst_case_scores:
        return np.inf  # If no inconsistencies exist, return a very high threshold.

    # Compute the (1 - alpha) quantile for thresholding
    tau_share = np.quantile(worst_case_scores, 1 - alpha)

    return tau_share

# Compute conformal threshold using the second half
tau_share = compute_conformal_threshold(cal_data, classifier, model, tokenizer, device, alpha=0.1)

# Print calibrated threshold
print(f"Calibrated stopping threshold: {tau_share:.4f}")

import matplotlib.pyplot as plt

probs = classifier.predict_proba(X_train)[:, 1]
plt.hist(probs, bins=20)
plt.xlabel("Stopping Probability")
plt.ylabel("Frequency")
plt.title("Distribution of Stopping Probabilities")
plt.show()

from transformers import StoppingCriteria, StoppingCriteriaList

class StopAtSentenceEnd(StoppingCriteria):
    """
    Custom stopping criteria to stop generation at sentence-ending punctuation.
    This ensures only one sentence is generated at a time.
    """
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        return any(decoded_text.endswith(punct) for punct in [".", "?", "!"])  # Stop at sentence end

for entry in test_data[:1]:
    question = entry["question"]

    print(f"\nProcessing test question: {question}")

    accumulated_text = ""  # Stores all generated sentences so far
    input_text = tokenizer(question, return_tensors="pt").to(device)

    # Define custom stopping criteria
    stopping_criteria = StoppingCriteriaList([StopAtSentenceEnd(tokenizer)])

    # Start iterative generation, sentence-by-sentence
    with torch.no_grad():
        for _ in range(50):  # Maximum steps to avoid infinite loops
            outputs = model.generate(
                **input_text,
                max_new_tokens=500,  # Generate a small chunk at a time
                stopping_criteria=stopping_criteria,  # Stop at sentence end
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=tokenizer.eos_token_id
            )

            # Decode the newly generated part
            new_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)

            # Extract the newly generated sentence
            new_sentences = new_text[len(accumulated_text):].strip().split(". ")
            if not new_sentences:
                break  # Stop if no valid sentence was generated

            # Take the first new sentence (ensuring it ends with proper punctuation)
            new_sentence = new_sentences[0].strip() + "."
            accumulated_text += " " + new_sentence if accumulated_text else new_sentence  # Append to accumulated text

            # Predict stopping probability using the accumulated text so far
            stopping_prob = classifier.predict_proba([accumulated_text.strip()])[:, 1][0]

            # Early stopping condition
            if stopping_prob > tau_share:
                final_answer = accumulated_text.strip()
                break  # Stop reasoning early
        else:
            # If no early stopping, return the full generated answer
            final_answer = accumulated_text.strip()

    # Print final result
    print(f"Final Answer: {final_answer}")

    # Generate full answer without early stopping
    with torch.no_grad():
        outputs = model.generate(
            **input_text,
            max_new_tokens=5000,  # Generate a longer response
            return_dict_in_generate=True,
            output_scores=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # Decode the full generated text
    full_answer = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)

    # Print the final answer (no early stopping)
    print(f"Final Answer (No Early Stopping): {full_answer.strip()}")